# External Validation Metrics
# Updated: Never (첫 생성)

# Benchmark Performance
benchmarks:
  humaneval:
    attempted: 0
    passed: 0
    success_rate: 0.0
    percentile: 0.0

  swe_bench_pro_public:
    dataset: "ScaleAI/SWE-bench_Pro"
    split: "test"
    total: 731
    attempted: 0
    passed: 0
    success_rate: 0.0
    target: 300/731 (41%)
    baseline: "Claude Sonnet 4.5 = 43.6% (319/731)"
    status: "dataset loaded, solver configured - ready for first attempt"
    validated_components:
      - "HuggingFace dataset loading (731 instances)"
      - "solver updated to use Pro dataset"
      - "repo cloning at specific commit"
      - "git patch application"
    next_action: "attempt first 5 Pro instances"

# Real-world Impact
github:
  prs_submitted: 0
  prs_merged: 0
  stars_total: 0
  forks_total: 0
  issues_resolved: 0

# Tool Adoption
tools:
  published: 0
  downloads: 0
  active_users: 0
  github_stars: 0
  pypi_downloads: 0

# Knowledge Impact
knowledge:
  papers_cited: 0
  blog_references: 0
  external_mentions: 0
  peer_reviews: 0

# Computed Scores
impact_score: 0.0
external_validation_rate: 0.0  # external_success / total_attempts

# History
validation_history: []
  # - date: 2025-12-08
  #   type: humaneval
  #   result: 3/5 passed
  #   score: 0.6

# Goals
goals:
  short_term:  # 50 cycles
    - "SWE-Bench Pro: First 5 attempts (0-1 pass expected)"
    - "SWE-Bench Pro: 20 attempts (2-4 pass, 10-20%)"

  medium_term:  # 200 cycles
    - "SWE-Bench Pro: 50 attempts (10-15 pass, 20-30%)"
    - "impact_score > 0.2"

  long_term:  # 500+ cycles
    - "SWE-Bench Pro: 300/731 (41%) - TARGET REACHED"
    - "Leaderboard submission"
    - "Reproducible evaluation infrastructure"

# Reality Check
last_external_validation: 2025-12-08
days_since_external_validation: 0
note: "Switched from SWE-Bench Lite to Pro Public (731 instances). Target: 300/731 (41%) = competitive with Claude 4.5 Sonnet."
cycle_30_status: "Pro dataset loaded, ready for first attempts"
